---
title: "Writing Functions"
author: "Katie Gao"
date: "11/30/2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse) 
library(rvest) 

knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  fig.width = 8, 
  fig.height = 6, 
  out.width = "90%"
  )

options(
  ggplot2.continuous.colour = "viridis", 
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d 

theme_set(theme_minimal() + theme(legend.position = "bottom")) 
```

## Get started 

We're going to write some functions. 

Here's z-scores: 
```{r}
x = rnorm(n = 30, mean = 4, sd = 2.3)
x_again = rnorm(n = 30, mean = 6, sd = .3)
y = rnorm(n = 30, mean = 24, sd = .3)

(x - mean(x)) / sd(x)
(x_again - mean(x_again)) / sd(x_again)
```

Now let's write a function: 
```{r}
## we're calling it the "z_score function" 
## it's a function of the variable x and the stuff that's going to be in the body of the function is in between these open curly brackets {}
## z_score is now going to be an object and that object is going to be a function 
## the argument into that function is something we're calling "x," also known as the "x argument into the function" 

z_score = function(x_arg) {
  
  if (!is.numeric(x_arg)) {
    stop("x should be numeric")
  } else if (length(x_arg) < 3) {
    stop("x should be longer than 3")
  }
  
  (x_arg - mean(x_arg)) / sd(x_arg) 
  
}

## if you go into the environment tab, we can see there's now a "z_score function" 
## so basically: we have a function that has a single argument ("x") and the 'x' inside the function will always look at what that argument is. It will look to see what the user told them what x to use
## highlight the x in the function to see where it is 
```

Let's try out the function:  
```{r, error = TRUE}
## these are z-transformations of the vectors that we've created 

z_score(x_arg = y) 

z_score(x_arg = 3) ## NA b/c can't compute the s.d. of one number 
z_score(x_arg = "my name is jeff") ## error b/c can't computer mean or s.d. of character vector 
z_score(x_arg = c(TRUE, TRUE, FALSE, TRUE)) ## does give us z-scores back 
z_score(x_arg = iris) ## doesn't work b/c it's a dataframe 
```

## Multiple outputs 

```{r}
## given one input vector, this spits out more than one output at the same time 

mean_and_sd = function(input_x) {
  
  if (!is.numeric(input_x)) {
    stop("x should be numeric")
  } else if (length(input_x) < 3) {
    stop("x should be longer than 3")
  } 
  
  list(
    mean_input = mean(input_x), 
    sd_input = sd(input_x), 
    z_score = (input_x - mean(input_x)) / sd(input_x) 
  ) 
  
  ## we create a tibble() to format these things 
  ## we can also put it in a list(), which allows us to keep track of things that are not just numbers 
  
}
```

Test this function: 

```{r}
mean_and_sd(input_x = y) 
```

## Multiple inputs  

```{r}
sim_data = tibble(
  x = rnorm(30, mean = 1, sd = 1),
  y = 2 + 3 * x + rnorm(30, 0, 1)
)
  ## we simulated x, and y is now (2 + 3x + noise on top of it)
  ## what we'd like to do is find the simple linear regression
  ## simple linear regression is meant to try to fit a line in the middle of points graphed to find the estimated intercept and estimated slope 

sim_data %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() 

ls_fit = lm(y ~ x, data = sim_data) ## 'lm' linear model function in R
  
beta0_hat = coef(ls_fit)[1] ## this gives us estimated intercept 
beta1_hat = coef(ls_fit)[2] ## this gives us estiamted slope 
``` 

Creating a simulation dataset 
```{r}
sim_regression = function(n) {
  
  sim_data = tibble(
    x = rnorm(n, mean = 1, sd = 1),
    y = 2 + 3 * x + rnorm(n, 0, 1)
  )

  ls_fit = lm(y ~ x, data = sim_data)  
  
  tibble(
    beta0_hat = coef(ls_fit)[1], 
    beta1_hat = coef(ls_fit)[2]
  )
  
}

sim_regression(n = 3000)

## it goes and creates a simulated dataframe of size 30
## it fits a linear regression b/w the y and the simulated dataset
## it returns a tibble with beta1-hat and beta0-hat 
## can run it over and over and over and get different estimates 
## the larger the sample size, the more accurate -- should be closer to 2 and 3 
```

Changing beta0 and beta1
```{r}
sim_regression = function(n, beta0, beta1) {
  
  sim_data = tibble(
    x = rnorm(n, mean = 1, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, 0, 1)
  ) 
  
  ## so now a user has to say: what is the intercept and what is the slope in my simulated dataset 
  ## what is the vertical shift and what is the slope that goes along with this 
  ## so the user now has to provide 3 arguments: sample size, true intercept, and true slope
  
  ls_fit = lm(y ~ x, data = sim_data)  
  
  tibble(
    beta0_hat = coef(ls_fit)[1], 
    beta1_hat = coef(ls_fit)[2]
  )
}

sim_regression(n = 3000, beta0 = 17, beta1 = -3)
## there are now 3 inputs: n, beta0, and beta1
## so now I get a simulated dataset of 3,000 subjects, it has a simulated intercept of 17, and a slope of -3
## I fiddle in the regression and try to extract those coefficients 


## can also use 'positional arguments' or 'positional matching'
sim_regression(3000, 2, 3)
```

Arguments can also have default values
```{r}
## by default, changing beta0 to 2 and beta1 to 3 
## so if the user doesn't do anything, those are the intercept and the slope right up until the user decides to do something different 

sim_regression = function(n, beta0 = 2, beta1 = 3) {
  
  sim_data = tibble(
    x = rnorm(n, mean = 1, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, 0, 1)
  ) 
  
  ls_fit = lm(y ~ x, data = sim_data)  
  
  tibble(
    beta0_hat = coef(ls_fit)[1], 
    beta1_hat = coef(ls_fit)[2]
  )
}

## this automatically assumes the intercept is 2 and slope is 3 
sim_regression(3000) 


## if the user wants to change this so that beta0 is 24, we can do that as well 
## you can change one of those arguments and not other/all of those arguments
sim_regression(n = 14, beta0 = 24)
```

So now we have 3 arguments in my function: sample size, intercept, and slope. My sample is going to generate a dataset with that sample size, and it's going to say that there's an x-variable and a y-variable and they are related through this intercept and slope. And all of those things are now dependent on whatever the user is putting in. 

Given that simulated dataset, we're fitting a simple linear regression and our function spits out a tibble with the estimated intercept and estimated slope based on that dataset. 









